{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61deb61f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Syntax-based Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e47584",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Given a token stream produced by a lexer, the objective of a parser is to construct a tree that captures the syntactic relation between the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01909fb5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CCLearner: Deep learning clone detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e239ce07",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We've already explored different code clone detection techniques, now it's time for yet another:\n",
    "\n",
    "Li, L., Feng, H., Zhuang, W., Meng, N., & Ryder, B. (2017, September). Cclearner: A deep learning-based clone detection approach. In 2017 IEEE International Conference on Software Maintenance and Evolution (ICSME) (pp. 249-260). IEEE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557e3c3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The idea of CCLearner is to compare code snippets in terms of the token frequencies. For this, the approach distinguishes the following token types:\n",
    "- Reserved words\n",
    "- Operators \n",
    "- Markers\n",
    "- Literals\n",
    "- Type identifiers\n",
    "- Method idenfiers\n",
    "- Qualified names\n",
    "- Variable identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0598280",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If you think about our tokenization approach from the last chapter, you'll realize there's a problem here: While we did distinguish between different types of tokens, our tokenizer cannot distinguish between type, method, qualified, and variable identifiers -- this was all just classified as \"identifier\". In order to distinguish these types of tokens we need to consider the syntactic context. This is what a parser does given a token stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb50df7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parsing programs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969a350",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will construct a simple parser for a trivial example language that resembles Python. Here's an example program:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed7637e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "def f(a, b)\n",
    "    add(100, add(20, add(a, b)))\n",
    "end\n",
    "\n",
    "print(f(1, 2))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b3d4d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "code = \"\"\"def f(a, b)\n",
    "    add(100, add(20, add(a, b)))\n",
    "end\n",
    "\n",
    "print(f(1, 2))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3324aa30",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The language contains function definitions, function calls, integer literals, and variable references. It also makes use of two undefined functions `add` and `print` which will be defined later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2fb59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b005d40",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A prerequisite for a parser is the token stream. We implemented a lexer in the last chapter in a very basic way, and in the end realised that what we had produced was an automaton matching regular expressions. We can thus implement a simpler lexer by defining the token types in terms of regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83110599",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b210fa7e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We won't actually need the position so let's just focus on lexemes and token types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e0103",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Token = namedtuple('Token', 'token_type value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc79f0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For our example language, we define the following token types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9bad92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "TOKEN_TYPES = [\n",
    "    (r'\\bdef\\b',        'def'),\n",
    "    (r'\\bend\\b',        'end'),\n",
    "    (r'\\b[a-zA-Z]+\\b',  'identifier'),\n",
    "    (r'\\b[0-9]+\\b',     'integer'),\n",
    "    (r'\\(',             'oparen'),\n",
    "    (r'\\)',             'cparen'),\n",
    "    (r',',              'comma'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f54e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(code):\n",
    "    tokens = []\n",
    "    \n",
    "    remaining_code = code\n",
    "    \n",
    "    while remaining_code:\n",
    "        for regex, token_type in TOKEN_TYPES:\n",
    "            match = re.match(regex, remaining_code)\n",
    "            if match:\n",
    "                value = match.group()\n",
    "                remaining_code = remaining_code[len(value):].strip()\n",
    "                tokens.append(Token(token_type, value))\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc036529",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tokenize(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de150e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9db1c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can now build a parser that constructs a parse tree, thus implicitly defining a grammar for our language. This is slightly more involved, so we will  construct this in an object oriented way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed021f1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = list(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84446f9d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The parser matches tokens based on a grammar. If the next token does not match a type allowed by the grammar, the parser reports an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ef830",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Parser(Parser):\n",
    "    def consume(self, expected_type):\n",
    "        token = self.tokens.pop(0)\n",
    "        if token.token_type == expected_type:\n",
    "            return token\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"Expected token type {expected_type!r} \"\n",
    "                f\"but got {token.token_type!r}.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3701987",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We now implicitly define our grammar by implementing each production as a function. Integer literals simply produce leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29266fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Parser(Parser):\n",
    "    def parse_integer(self):\n",
    "        return dict(\n",
    "            node_type='int',\n",
    "            value=int(self.consume('integer').value),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01229e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "example = \"5\"\n",
    "parser = Parser(tokenize(example))\n",
    "parser.parse_integer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d0216a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Similarly, variable nodes are leaf nodes containing the variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2c5e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Parser(Parser):\n",
    "    def parse_var_ref(self):\n",
    "        return dict(\n",
    "            node_type='var',\n",
    "            name=self.consume('identifier').value,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee687ef6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "example = \"x\"\n",
    "parser = Parser(tokenize(example))\n",
    "parser.parse_var_ref()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623aa7b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Function calls are slightly more involved since they are not just individual tokens. To determine which grammar rule we are matching we sometimes need to look ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd6ab32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Parser(Parser):\n",
    "    def peek(self, expected_type, offset=0):\n",
    "        return self.tokens[offset].token_type == expected_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c5f2b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A function call consists of a call node that contains the name of the function called, and nodes for the arguments, if there are any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611cc12",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Parser(Parser):\n",
    "    def parse_call(self):\n",
    "        name = self.consume('identifier').value\n",
    "        arg_exprs = list(self.parse_arg_exprs())\n",
    "        return dict(\n",
    "            node_type='call',\n",
    "            name=name,\n",
    "            arg_exprs=arg_exprs,\\\n",
    "        )\n",
    "\n",
    "    def parse_arg_exprs(self):\n",
    "        self.consume('oparen')\n",
    "        if not self.peek('cparen'):\n",
    "            yield self.parse_expr()\n",
    "            while self.peek('comma'):\n",
    "                self.consume('comma')\n",
    "                yield self.parse_expr()\n",
    "        self.consume('cparen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23cbdc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Parser(Parser):\n",
    "    def parse_expr(self):\n",
    "        if self.peek('integer'):\n",
    "            return self.parse_integer()\n",
    "        elif self.peek('identifier') and self.peek('oparen', 1):\n",
    "            return self.parse_call()\n",
    "        else:\n",
    "            return self.parse_var_ref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e0b018",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "example = \"foo(5)\"\n",
    "parser = Parser(tokenize(example))\n",
    "parser.parse_call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71beefe2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Parser(Parser):\n",
    "    def parse_def(self):\n",
    "        self.consume('def')\n",
    "        name = self.consume('identifier').value\n",
    "        arg_names = list(self.parse_arg_names())\n",
    "        body = self.parse_expr()\n",
    "        self.consume('end')\n",
    "        return dict(\n",
    "            node_type='def',\n",
    "            name=name,\n",
    "            arg_names=arg_names,\n",
    "            body=body,\n",
    "        )\n",
    "\n",
    "    def parse_arg_names(self):\n",
    "        self.consume('oparen')\n",
    "        if self.peek('identifier'):\n",
    "            yield self.consume('identifier').value\n",
    "            while self.peek('comma'):\n",
    "                self.consume('comma')\n",
    "                yield self.consume('identifier').value\n",
    "        self.consume('cparen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d10a29",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "parser = Parser(tokenize(code))\n",
    "parser.parse_def()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f33d67",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Finally, we need to implement the start rule of our grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7cc570",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Parser(Parser):\n",
    "    def parse(self):\n",
    "        while self.tokens:\n",
    "            if self.peek('def'):\n",
    "                yield self.parse_def()\n",
    "            else:\n",
    "                yield self.parse_call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd8948",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tree = Parser(tokenize(code)).parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78127e2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "list(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca6d89",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can visualize the syntax tree using a little helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5314dd43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "unique_id = 0\n",
    "\n",
    "def print_tree(node):\n",
    "    dot = Digraph()\n",
    "    num = 0\n",
    "    dot.node(\"root\", \"start\")\n",
    "    for child in list(node):\n",
    "        child_id = add_nodes(dot, child, f\"root-{num}\")\n",
    "        dot.edge(\"root\", child_id)\n",
    "        num += 1\n",
    "    return dot\n",
    "\n",
    "def add_nodes(dot, node, name):\n",
    "    global unique_id\n",
    "   \n",
    "    if isinstance(node, dict):\n",
    "        dot.node(str(id(node)), name)\n",
    "        for key, value in node.items():\n",
    "            child_id = add_nodes(dot, value, key)\n",
    "            dot.edge(str(id(node)), child_id)\n",
    "        return str(id(node))\n",
    "                \n",
    "    elif isinstance(node, str):\n",
    "            node_id = unique_id\n",
    "            unique_id += 1\n",
    "            dot.node(str(node_id), node)\n",
    "            return str(node_id)\n",
    "\n",
    "    elif isinstance(node, int):\n",
    "            node_id = unique_id\n",
    "            unique_id += 1\n",
    "            dot.node(str(node_id), str(node))\n",
    "            return str(node_id)\n",
    "\n",
    "    elif isinstance(node, list):\n",
    "        dot.node(str(id(node)), name)\n",
    "        num = 0\n",
    "        for child in node:\n",
    "            child_id = add_nodes(dot, child, f\"{name}-{num}\")\n",
    "            dot.edge(str(id(node)), child_id)\n",
    "            num += 1\n",
    "        return str(id(node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8de73d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "root_nodes = list(Parser(tokenize(code)).parse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99cc35",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print_tree(root_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32295d01",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that this is not yet an _abstract_ syntax tree: It is a parse tree, exactly representing the grammar used, including all tokens. In contrast, an abstract syntax tree describes the parse tree logically and does not need to contain all the syntactical constructs. While a parse tree only has non-terminal nodes as non-leaf nodes, an abstract syntax tree can, for example, contain operators as interor nodes, with the operands being leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1a4ab2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Revisiting CCLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af0b68",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If we want to parse real languages, we'll often find existing parsers. To process Java code in Python, we can use Javalang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0738f522",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "code1 = \"\"\"\n",
    "public class Foo {\n",
    "  public void foo(int x) {\n",
    "    System.out.println(\"Hello Clone!\");\n",
    "    int j = 10;\n",
    "    for(int i = 0; i < x; i++) {\n",
    "      System.out.println(\"Another iteration\");\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1da541",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "code2 = \"\"\"\n",
    "public class Foo {\n",
    "  private int y = 0;\n",
    "  \n",
    "  public void foo(int x) {\n",
    "    System.out.println(\"Hello Clone!\");\n",
    "    int j = 10 + y;\n",
    "    for(int i = 0; i < x; i++) {\n",
    "      System.out.println(\"Another iteration\");\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08e300",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import javalang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ef69d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "javalang.parse.parse(code2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934808be",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It might be helpful to see the tree structure visualised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1dc930",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "def print_tree(tree):\n",
    "    unique_id = 1\n",
    "    dot = Digraph()\n",
    "    for path, node in tree:\n",
    "        dot.node(str(id(node)), str(type(node)))\n",
    "        \n",
    "        for child in node.children:\n",
    "            if isinstance(child, javalang.ast.Node):\n",
    "                dot.edge(str(id(node)), str(id(child)))\n",
    "            elif type(child) == str:\n",
    "                strid = str(unique_id)\n",
    "                unique_id = unique_id + 1\n",
    "                dot.node(strid, child)\n",
    "                dot.edge(str(id(node)), strid)\n",
    "            elif type(child) == list:\n",
    "                for lc in child:\n",
    "                    dot.edge(str(id(node)), str(id(lc)))\n",
    "                 \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624a6d8",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tree = javalang.parse.parse(code2)\n",
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55622baa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In contrast to our parse tree shown earlier, this is an actual _abstract_ syntax tree. To construct an AST, one needs to extend the implementations of the different productions to instantiate the appropriate node structures required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff362f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "CCLearner defines eight different types of tokens for the clone analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c534d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "reserved   = {} # C1\n",
    "operators  = {} # C2\n",
    "markers    = {} # C3\n",
    "literals   = {} # C4\n",
    "type_ids   = {} # C5\n",
    "method_ids = {} # C6\n",
    "qualified_ids = {} # C7\n",
    "variable_ids  = {} # C8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e94232",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def increment(dictionary, key):\n",
    "    if key in dictionary:\n",
    "        dictionary[key] += 1\n",
    "    else:\n",
    "        dictionary[key] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cb6679",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The first three types of tokens can easily be extracted using a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46282aa4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for token in javalang.tokenizer.tokenize(code2):\n",
    "    # C1\n",
    "    if token.__class__.__name__ == \"Keyword\":\n",
    "        increment(reserved, token.value)\n",
    "\n",
    "    # C2\n",
    "    elif token.__class__.__name__ == \"Operator\":\n",
    "        increment(operators, token.value)\n",
    "    \n",
    "    # C3\n",
    "    elif token.__class__.__name__ == \"Separator\":\n",
    "        increment(markers, token.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d8f4e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Type C4 (Literals) already comes with some challenges. For example, consider the following snippet of code and its tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25b832",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "list(javalang.tokenizer.tokenize(\"int i = -1;\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5305ad23",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The number `-1` is split into two tokens, but for the sake of CCLearner's analysis it would be preferable to use a single number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04ccc2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To extract literals values, we can, however, use the AST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a5df3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "example_tree = javalang.parse.parse(\"class Test {int i = -1;}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a7efa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for _, node in example_tree.filter(javalang.tree.Literal):\n",
    "    print(f\"Literal: {node}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7627aa4e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We thus need to prepend the prefix operators when collecting literals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d84da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for _, node in tree.filter(javalang.tree.Literal):\n",
    "    result = \"\".join(node.prefix_operators) + node.value\n",
    "    # C4\n",
    "    increment(literals, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f8c2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for _, node in tree.filter(javalang.tree.Type):\n",
    "    # C5\n",
    "    increment(type_ids, node.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961815e9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For C6 we require all method names, which are part of MethodDeclarations and MethodInvocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd6ea68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for _, node in tree.filter(javalang.tree.MethodInvocation):\n",
    "    # C6\n",
    "    increment(method_ids, node.member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cbfccf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for _, node in tree.filter(javalang.tree.MethodDeclaration):\n",
    "    # C6\n",
    "    increment(method_ids, node.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5985a53b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Qualified names (C7 tokens) are explicitly available in the AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d0f116",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for _, node in tree.filter(javalang.tree.Primary):\n",
    "    # C7\n",
    "    if node.qualifier:\n",
    "        increment(qualified_ids, node.qualifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10998df",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Variable identifiers (C8 tokens) are slightly more inconvenient to extract than the other tokens because they can occur at multiple different types of locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d363fba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for _, node in tree.filter(javalang.tree.VariableDeclarator):\n",
    "    # C8\n",
    "    increment(variable_ids, node.name)\n",
    "\n",
    "for _, node in tree.filter(javalang.tree.FormalParameter):\n",
    "    # C8\n",
    "    increment(variable_ids, node.name)\n",
    "    \n",
    "for _, node in tree.filter(javalang.tree.MemberReference):\n",
    "    # C8\n",
    "    increment(variable_ids, node.member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce3dd3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(reserved)\n",
    "print(operators)\n",
    "print(markers)\n",
    "print(literals)\n",
    "print(type_ids)\n",
    "print(method_ids)\n",
    "print(qualified_ids)\n",
    "print(variable_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d264d69",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we can place all the conditions from above into a function that derives the tokens for a given snippet of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e7113",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_tokens(code):\n",
    "    \n",
    "    tokens = { \n",
    "        \"reserved\" : {},\n",
    "        \"operators\" : {},\n",
    "        \"markers\" : {},\n",
    "        \"literals\" : {},\n",
    "        \"type_ids\" : {},\n",
    "        \"method_ids\" : {},\n",
    "        \"qualified_ids\" : {},\n",
    "        \"variable_ids\" : {}\n",
    "             }\n",
    "\n",
    "    for token in javalang.tokenizer.tokenize(code):\n",
    "        # C1\n",
    "        if token.__class__.__name__ == \"Keyword\":\n",
    "            increment(tokens[\"reserved\"], token.value)\n",
    "        # C2\n",
    "        elif token.__class__.__name__ == \"Operator\":\n",
    "            increment(tokens[\"operators\"], token.value)    \n",
    "        # C3\n",
    "        elif token.__class__.__name__ == \"Separator\":\n",
    "            increment(tokens[\"markers\"], token.value)\n",
    "\n",
    "    tree = javalang.parse.parse(code)\n",
    "    for _, node in tree.filter(javalang.tree.Literal):\n",
    "        result = \"\".join(node.prefix_operators) + node.value\n",
    "        # C4\n",
    "        increment(tokens[\"literals\"], result)\n",
    "    for _, node in tree.filter(javalang.tree.Type):\n",
    "        # C5\n",
    "        increment(tokens[\"type_ids\"], result)\n",
    "    for _, node in tree.filter(javalang.tree.MethodInvocation):\n",
    "        # C6\n",
    "        increment(tokens[\"method_ids\"], node.member)\n",
    "    for _, node in tree.filter(javalang.tree.MethodDeclaration):\n",
    "        # C6\n",
    "        increment(tokens[\"method_ids\"], node.name)\n",
    "    for _, node in tree.filter(javalang.tree.Primary):\n",
    "        # C7\n",
    "        if node.qualifier:\n",
    "            increment(tokens[\"qualified_ids\"], node.qualifier)\n",
    "    for _, node in tree.filter(javalang.tree.VariableDeclarator):\n",
    "        # C8\n",
    "        increment(tokens[\"variable_ids\"], node.name)\n",
    "    for _, node in tree.filter(javalang.tree.FormalParameter):\n",
    "        # C8\n",
    "        increment(tokens[\"variable_ids\"], node.name)\n",
    "    for _, node in tree.filter(javalang.tree.MemberReference):\n",
    "        increment(tokens[\"variable_ids\"], node.member)\n",
    "        # C8\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2419933c",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "get_tokens(code1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59b70f9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The similarity for a given cataegory tokens is calculated as 1 minus the difference of token frequencies over the sums of token frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e494bd2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def sim_score(tokens1, tokens2):\n",
    "    if not tokens1 or not tokens2:\n",
    "        return 0.5 # See paper\n",
    "    \n",
    "    tokens = list(tokens1.keys()) + list(tokens2.keys())\n",
    "    \n",
    "    diff = 0\n",
    "    summ = 0 \n",
    "    \n",
    "    for token in tokens:\n",
    "        num1 = tokens1[token] if token in tokens1 else 0\n",
    "        num2 = tokens2[token] if token in tokens2 else 0\n",
    "        diff += num1 - num2\n",
    "        summ += num1 + num2\n",
    "    \n",
    "    return 1.0 - diff / summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd5bcee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "code3 = \"\"\"\n",
    "public class Bar {\n",
    "  public void bar(int x) {\n",
    "    System.out.println(\"Completely different text!\");\n",
    "    int j = 200; // completely different numbers\n",
    "    for(int i = 100; i < x; i++) {\n",
    "      System.out.println(\"More complete different text\");\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dcd70c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "code4 = \"\"\"\n",
    "public class Bar {\n",
    "  public void bar(int x) {\n",
    "        MultiLayerNetwork model = new MultiLayerNetwork(conf);\n",
    "        model.init();\n",
    "        model.setListeners(Collections.singletonList((IterationListener) new ScoreIterationListener(10)));\n",
    "\n",
    "\n",
    "        for (int n = 0; n < nEpochs; n++) {\n",
    "            model.fit(trainIter);\n",
    "        }\n",
    "\n",
    "        File model_File = new File(output_dir + \"model.mdl\");\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d22913",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tokens1 = get_tokens(code1)\n",
    "tokens2 = get_tokens(code2)\n",
    "tokens3 = get_tokens(code3)\n",
    "tokens4 = get_tokens(code4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba6a59",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sim_score(tokens1[\"markers\"], tokens2[\"markers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d533708",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def feature_vector(tokens1, tokens2):\n",
    "    similarity = []\n",
    "    for key in tokens1.keys():\n",
    "        similarity.append(sim_score(tokens1[key], tokens2[key]))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49646b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "feature_vector(tokens1, tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca17a0e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "feature_vector(tokens1, tokens4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b82df2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "At this point, we can train a machine learning model given a labelled dataset of pairs of code snippets. For each pair of code snippets we would calculate the feature vector, and then update train the model based on the label for that pair. However, rather than doing this, we will take our prediction even a step further and use more syntactical information for our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2d40b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Code Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22daed00",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Before moving on to somewhat more classical examples of syntax-based analysis of code, we consider a recent trend: In order to allow machine learning models to make predictions on code, we need to convert the source code to a format that is suitable as input for ML models, i.e., numerical vectors, typically referred to as _embeddings_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b80b1d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A well-known example of embeddings in a different domain is Word2vec: Word2vec is a two-layer neural net that processes text by “vectorizing” words. Its input is a text corpus and its output is a set of vectors: feature vectors that represent words in that corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a63033",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The purpose and usefulness of Word2vec is to group the vectors of similar words together in vectorspace. That is, it detects similarities mathematically. Word2vec creates vectors that are distributed numerical representations of word features, features such as the context of individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bcb469",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Given enough data, usage and contexts, Word2vec can make highly accurate guesses about a word’s meaning based on past appearances. Those guesses can be used to establish a word’s association with other words (e.g. “man” is to “boy” what “woman” is to “girl”), or cluster documents and classify them by topic. Those clusters can form the basis of search, sentiment analysis and recommendations in such diverse fields as scientific research, legal discovery, e-commerce and customer relationship management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39c9ac",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The output of the Word2vec neural net is a vocabulary in which each item has a vector attached to it, which can be fed into a deep-learning net or simply queried to detect relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b894259",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6bd4d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "glove_vectors.most_similar('twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b999f033",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ASTNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8003fab6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Recent work provides the strong evidence that syntactic knowledge contributes more in modeling source code\n",
    "and can obtain better representation than traditional token-based methods. We will consider one example approach to creating code embeddings from syntactic information, ASTNN:\n",
    "\n",
    "Zhang, J., Wang, X., Zhang, H., Sun, H., Wang, K., & Liu, X. (2019, May). A novel neural source code representation based on abstract syntax tree. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE) (pp. 783-794). IEEE.\n",
    "\n",
    "ASTNN splits the large AST of one code fragment into a set of small trees at the statement level and performs tree-based neural embeddings on all statement trees. Recurrent Neural Networks (RNNs) are used to encode statements and the sequential dependency between the statements into a vector. These vectors capture the naturalness of source code, and can serve as a neural source code representation.\n",
    "\n",
    "\n",
    "As an example application for these embeddings, we can once again use code clone detection, which boils down to the following:\n",
    "\n",
    "- Compute vector embeddings $e_1$, $e_2 \\in \\mathbb{R}^m$ for two code snippets\n",
    "- The distance between the code snippets is $r = |e_1 - e_2| \\in \\mathbb{R}^m$\n",
    "- This can be reduced to a clone probability using a linear layer with sigmoid activation function $p = \\textrm{sigmoid}(r) \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d2521",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8558472",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The first step in producing the code embeddings consists of parsing the code, transforming the AST into a sequence of statement trees, and then replacing string labels of the tree nodes with numeric indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee2863c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will eventually apply our encoding to a dataset of C programs (using the model trained by the authors of ASTNN), so in the following we will consider the syntax trees of C programs created by Python's C Parser library: [PyCParser](https://github.com/eliben/pycparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c19f98",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "int foo() {}\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "  if (argc > 0) {\n",
    "      foo();\n",
    "  }\n",
    "  return 0;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2452eff4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pycparser\n",
    "code_parser = pycparser.c_parser.CParser()\n",
    "\n",
    "ast = code_parser.parse(code)\n",
    "ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7539efa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "At the core of the ASTNN lies the extraction of statement trees from the AST. A statement tree is essentially a substree of the AST for a statement-node, and the list of statement trees is achieved by a preorder traversal of the AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e176d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_statements(node):\n",
    "    name = node.__class__.__name__\n",
    "    print(f\"Current node: {name}\")\n",
    "    for _, child in node.children():\n",
    "        get_statements(child)\n",
    "    \n",
    "get_statements(ast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0a857",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For our example program, we would like to create statement trees for the if statement (`If`), the function call (`FuncCall`) , and the return statement (`Return`). ASTNN also treats function declarations (`FuncDef`) as special statement nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ecfed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_statements(node):\n",
    "    name = node.__class__.__name__\n",
    "    if name in [\"FuncDef\", \"FuncCall\", \"If\", \"Return\"]:\n",
    "        print(f\"Statement node: {name}\")\n",
    "    for _, child in node.children():\n",
    "        get_statements(child)\n",
    "    \n",
    "get_statements(ast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0de7ca",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A statement tree (ST-tree) rooted by the statement node s ∈ S is the tree consisting of node s and all of\n",
    "its descendants, but excluding any statements nodes. For example, for a method call statement, all child nodes are in the statement tree, since none of the children are statements. On the other hand, an if-statement consists of an expression as well as the conditionally executed statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e25e22",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "code_parser.parse(\"int main() {if (42 > 0) { foo(); } }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa29cc76",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Consequently, when creating statement trees, if we encounter a `FuncDef`, `If`, `While`, `DoWhile`, or `Switch` statement, then we only include the first child in the statement tree, and ignore all other children (which are statements)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc282212",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A second exception are for-loops, which contain of multiple children: The `init`, `cond`, and `next` children are part of the for-statement itself, while the last child (`stmt`) is a statement and should be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefea982",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "code_parser.parse(\"int main() {for (int i = 0; i < 10; i++) { foo(); } }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b744d0e0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note that the for-loop (and the other block-constructs we considered previously) define a single statement as child node, but of course it is common that they can contain more complex code blocks. In pycparser, these are are captured by `Compound` nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be8c54",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It is also worth noting that pycparser stores nodes as tuples `(str, node)`, so when traversing the AST to create statement trees we need to look at the second entry of such a tuple only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7472b29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ASTNode(object):\n",
    "    def __init__(self, node):\n",
    "        self.node = node\n",
    "        self.name = self.node.__class__.__name__\n",
    "        children = node.children()\n",
    "        if self.name in ['FuncDef', 'If', 'While', 'DoWhile', 'Switch']:\n",
    "            self.__children = [ASTNode(children[0][1])]\n",
    "        elif self.name == 'For':\n",
    "            children = node.children()\n",
    "            self.__children = [ASTNode(children[c][1]) for c in range(0, len(children) - 1)]\n",
    "        else:\n",
    "            self.__children = [ASTNode(child) for _, child in node.children()]\n",
    "\n",
    "    def children(self):\n",
    "        return self.__children\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.name}: {self.children()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac052690",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now that we have a class to capture statement trees, we just need to implement the tree traversal to collect them for the statements we are interested (for the sake of the example for now only `FuncDef`, `FuncCall`, `If`, `Return`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7e065",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_statement_trees(node):\n",
    "    name = node.__class__.__name__\n",
    "    trees = []\n",
    "    if name in [\"FuncDef\", \"FuncCall\", \"If\", \"Return\"]:\n",
    "        trees.append(ASTNode(node))\n",
    "\n",
    "    for _, child in node.children():\n",
    "        trees.extend(get_statement_trees(child))\n",
    "    \n",
    "    return trees\n",
    "    \n",
    "get_statement_trees(ast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3199d470",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our statement trees currently only describe the syntactic structure, but we have lost the lexical information about the actual tokens used (e.g., which methods were called). We add a `token` to our `ASTNode` class.\n",
    "- If the node is a leaf node (i.e., a variable name or a literal), then we use the actual lexeme.\n",
    "- For type declaration nodes we use the name of the type.\n",
    "- For operators, we use the operator symbol.\n",
    "- In other cases, we use the token type as the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e12ec5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ASTNode(ASTNode):\n",
    "    def __init__(self, node):\n",
    "        super().__init__(node)\n",
    "        self.token = self.get_token()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.token}: {self.children()}\"\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return len(self.node.children()) == 0    \n",
    "    \n",
    "    def get_token(self, lower=True):\n",
    "        name = self.node.__class__.__name__\n",
    "        token = name\n",
    "\n",
    "        if self.is_leaf():\n",
    "            attr_names = self.node.attr_names\n",
    "            if 'names' in attr_names:\n",
    "                token = self.node.names[0] # Identifiers\n",
    "            elif 'name' in attr_names:\n",
    "                token = self.node.name # ID\n",
    "            else:\n",
    "                token = self.node.value # Constant\n",
    "        else:\n",
    "            if name == 'TypeDecl':\n",
    "                token = self.node.declname\n",
    "            if self.node.attr_names:\n",
    "                attr_names = self.node.attr_names\n",
    "                if 'op' in attr_names:\n",
    "                    token = self.node.op # Binary Op\n",
    "\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df927c70",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "get_statement_trees(ast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ab761f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Since a neural network cannot process string labels of the trees, we first need to convert these to integers by taking their index in a vocabulary. In ASTNN this is done using a pre-trained word2vec vocabulary. If the label cannot be found, then an out-of-vocabulary index is assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857561d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The word2vec model is trained on the source code corpus; we will simply use the model generated by the ASTNN authors here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f401eb62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec.load(\"data/astnn/w2v_128\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7fa97c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The index of a word can be determined by directly looking the word up in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247f3d92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def label_to_index(label: str) -> int:\n",
    "    return w2v.wv.get_index(label, default=len(w2v.wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea45c044",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To convert the labels of a statement tree to numbers, we apply this to each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d57e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def tree_to_index(node: ASTNode):\n",
    "    token = node.token\n",
    "    indices = [label_to_index(token)]\n",
    "    for child in node.children():\n",
    "        indices.append(tree_to_index(child))\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19114a7a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's have a look at the statement trees for our example snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c8af1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import pycparser\n",
    "\n",
    "for s in get_statement_trees(ast):\n",
    "    print(tree_to_index(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22be4a2b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our implementation of the AST traversal is limited to our example code snippet, and will not work on more general code snippets. To apply this to any C code snippets, let's use the full version, which mainly differs in which aspects of the AST it takes into consideration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d74a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ast_block_token = ['FuncDef', 'If', 'While', 'DoWhile', 'Switch']\n",
    "\n",
    "\n",
    "class ASTNode(object):\n",
    "    def __init__(self, node, single=False):\n",
    "        self.node = node\n",
    "\n",
    "        self.__is_str = isinstance(self.node, str)\n",
    "        self.token = self.get_token()\n",
    "\n",
    "        if single:\n",
    "            self.__children = []\n",
    "        else:\n",
    "            self.__children = self.add_children()\n",
    "\n",
    "    def is_leaf(self):\n",
    "        if self.__is_str:\n",
    "            return True\n",
    "\n",
    "        return len(self.node.children()) == 0\n",
    "\n",
    "    def add_children(self):\n",
    "        if self.__is_str:\n",
    "            return []\n",
    "        children = self.node.children()\n",
    "        if self.token in ast_block_token:\n",
    "            return [ASTNode(children[0][1])]\n",
    "        elif self.token == 'For':\n",
    "            return [ASTNode(children[c][1]) for c in range(0, len(children) - 1)]\n",
    "        else:\n",
    "            return [ASTNode(child) for _, child in children]\n",
    "\n",
    "    def children(self):\n",
    "        return self.__children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f7d83d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The retrieval of the right token is also slightly more involved beyond our snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5be34e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class ASTNode(ASTNode):\n",
    "    def get_token(self, lower=True):\n",
    "        if self.__is_str:\n",
    "            return self.node\n",
    "\n",
    "        name = self.node.__class__.__name__\n",
    "        token = name\n",
    "        is_name = False\n",
    "\n",
    "        if self.is_leaf():\n",
    "            attr_names = self.node.attr_names\n",
    "            if attr_names:\n",
    "                if 'names' in attr_names:\n",
    "                    token = self.node.names[0]\n",
    "                elif 'name' in attr_names:\n",
    "                    token = self.node.name\n",
    "                    is_name = True\n",
    "                else:\n",
    "                    token = self.node.value\n",
    "            else:\n",
    "                token = name\n",
    "        else:\n",
    "            if name == 'TypeDecl':\n",
    "                token = self.node.declname\n",
    "            if self.node.attr_names:\n",
    "                attr_names = self.node.attr_names\n",
    "                if 'op' in attr_names:\n",
    "                    if self.node.op[0] == 'p':\n",
    "                        token = self.node.op[1:]\n",
    "                    else:\n",
    "                        token = self.node.op\n",
    "        if token is None:\n",
    "            token = name\n",
    "        if lower and is_name:\n",
    "            token = token.lower()\n",
    "        return token\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.get_token()}: {self.children()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec816a22",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Finally, our retrieval of statement trees was slightly simplified. For example, the original ASTNN implementation also creates nodes for compound statements, and adds dedicated `End` nodes. These end-nodes do not match lexical tokens but inform the inference algorithm about the indentation of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86c464",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_statements(node, statement_sequence):\n",
    "    children = node.children()\n",
    "    name = node.__class__.__name__\n",
    "    if name in ['FuncDef', 'If', 'For', 'While', 'DoWhile']:\n",
    "        statement_sequence.append(ASTNode(node))\n",
    "        if name != 'For':\n",
    "            inner_offset = 1\n",
    "        else:\n",
    "            inner_offset = len(children) - 1\n",
    "        for i in range(inner_offset, len(children)):\n",
    "            child = children[i][1]\n",
    "            if child.__class__.__name__ not in ['FuncDef', 'If', 'For', 'While', 'DoWhile', 'Compound']:\n",
    "                statement_sequence.append(ASTNode(child))\n",
    "            get_statements(child, statement_sequence)\n",
    "    elif name == 'Compound':\n",
    "        statement_sequence.append(ASTNode(name))\n",
    "        for _, child in children:\n",
    "            if child.__class__.__name__ not in ['If', 'For', 'While', 'DoWhile']:\n",
    "                statement_sequence.append(ASTNode(child))\n",
    "            get_statements(child, statement_sequence)\n",
    "        statement_sequence.append(ASTNode('End'))\n",
    "    else:\n",
    "        for _, child in children:\n",
    "            get_statements(child, statement_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb631cd5",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "statements = []\n",
    "get_statements(ast, statements)\n",
    "statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab54a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def to_statement_trees(ast) -> list[Any]:\n",
    "    statements = []\n",
    "    get_statements(ast, statements)\n",
    "    tree = []\n",
    "    for s in statements:\n",
    "        tree.append(tree_to_index(s))\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962933ff",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956164da",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Given the numeric encoding of tokens in the statement trees, the next step of building the embeddings consists of recursively creating vectors for statement trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fe2d92",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The lexical vector $v_n$ for a node $n$ is calculated as $v_n = W_e^T x_n$, where $x_n$ is the numerical representation of node $n$ (i.e., the index retrieved from Word2vec)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c00742",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The vector representation of a node $n$ is computed by the following equation:\n",
    "\n",
    "$h = \\sigma (W_n^T + \\displaystyle\\sum_{i \\in [1, C]} h_i + b_n)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dd334e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here, $W_n \\in \\mathcal{R}^{d \\times k}$ is the weight matrix with encoding dimensions $k$.  $b_n$ is a bias term, $h_i$ is the hidden state for each child $i$, $h$ is the updated hidden state, and $\\sigma$ is the activation function, which in ASTNN is the identity function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3065ab",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The final encoding is then sampled with max pooling: $e_t = [max(h_{i1}, \\ldots, max(h_{ik})], i = 1, \\ldots, N$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60bce5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dynamic Batching of trees (Algorithm 1, function `DynamicBatch()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc7c920",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The computation of the vector encoding of a statement tree recursively depends on the vector encoding of its subnodes in the tree. Furthermore, For example, directly calculating $h$ for the two parents in one batch may also be impossible if the number of children of the two parents differs. This means that the computation of $h$ cannot be parallelised, but unfortunately that is exactly what is necessary in order to train a model on a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16252e63",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The authors of ASTNN have developed a batch processing algorithm that allows to encode multiple samples (i.e., code fragments) simultaneously. However, generally batch processing on multiway ST-trees makes it difficult since the number of children nodes varies for the parent nodes in the same position of one batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346f6e1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The algorithm batches $L$ samples of statement trees and then breadthfirst traverses them starting from the root nodes. For the current nodes $ns$ in the same position of the batch, the algorithm first calculates $v_n$ in batch, and then group all their children nodes by the node positions. That is, it groups all nodes at the same depth in the statement trees at the same position, such that the calculation of these nodes can be performed in parallel. Based on the grouping, the algorithm recursively performs batch processing on all children nodes. After getting the results of all children nodes, $h$ can be computed in batch, and all node vectors of calculated statement trees are recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32951f33",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Children Batching](figures/5_astnn_children_batching.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f0060",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Children batching\n",
    "\n",
    "- $ns$: `nodes`\n",
    "- $C$: `children`\n",
    "- $CI$: `children_index`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ee9ba9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Since our focus lies in the program analysis itself more than on the construction of an effective machine learning pipeline, we will present the ML-related code here, but will not go into as much detail as for the previous parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff70c8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The following code sets up the batch tree encoder used by ASTNN, and initially sets up the required datastructures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32472a96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy.typing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from typing import Optional, Union\n",
    "\n",
    "\n",
    "class BatchTreeEncoder(tf.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        vocab_embedding_dim: int,\n",
    "        encode_dim: int,\n",
    "        batch_size: int = 64,\n",
    "        pretrained_weight: Optional[numpy.typing.ArrayLike] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, vocab_embedding_dim)\n",
    "        self.embedding_dim = vocab_embedding_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        self.W_c = keras.layers.Dense(encode_dim, input_shape=(vocab_embedding_dim,))\n",
    "        self.activation = tf.keras.activations.relu\n",
    "        self.batch_size = batch_size\n",
    "        self.node_list: list[tf.Tensor] = []\n",
    "        self.batch_node: Union[list[int], tf.Tensor] = []\n",
    "\n",
    "        # pretrained embedding from word2vec\n",
    "        if pretrained_weight is not None:\n",
    "            self.embedding.build((vocab_size, vocab_embedding_dim))\n",
    "            self.embedding.set_weights([pretrained_weight])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1deb74e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The actual traversal implements Algorithm 1 from the ASTNN paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54226cc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class BatchTreeEncoder(BatchTreeEncoder):\n",
    "    def traverse(self, nodes, batch_index: list[int]) -> Optional[tf.Tensor]:\n",
    "        # Recursively compute embedding of multiple statement trees `nodes`\n",
    "        size = len(nodes)\n",
    "        if not size:\n",
    "            return None\n",
    "\n",
    "        # line 9: create an output placeholder `BC` for the batch input\n",
    "        batch_current = tf.zeros([size, self.embedding_dim], tf.float32)\n",
    "\n",
    "        index: list[int] = []\n",
    "        current_node: list[int] = []\n",
    "        children: list[list[int]] = []\n",
    "        children_index: list[list[int]] = []\n",
    "\n",
    "        for i, n in enumerate(nodes):\n",
    "            index.append(i)\n",
    "            current_node.append(n[0])\n",
    "\n",
    "            for j, child in enumerate(n[1:]):\n",
    "                # check if the children actually has a valid token index\n",
    "                if child[0] == -1:\n",
    "                    continue\n",
    "                # line 14: group children by their position\n",
    "                if len(children_index) <= j:\n",
    "                    children_index.append([i])\n",
    "                    children.append([child])\n",
    "                else:\n",
    "                    children_index[j].append(i)\n",
    "                    children[j].append(child)\n",
    "\n",
    "        index = tf.expand_dims(index, axis=-1)\n",
    "\n",
    "        batch_current = self._recurse(batch_current, batch_index, children, children_index, current_node, index, size, )\n",
    "        self._update_node_list(batch_current, batch_index)\n",
    "\n",
    "        return batch_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a3a8ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class BatchTreeEncoder(BatchTreeEncoder):\n",
    "    def _recurse(self, batch_current, batch_index, children, children_index, current_node, index, size, ):\n",
    "        # line 10: Equation 1\n",
    "        batch_current = self.W_c(\n",
    "            tf.tensor_scatter_nd_update(batch_current, index, self.embedding(tf.Variable(current_node)))\n",
    "        )\n",
    "\n",
    "        # line 17\n",
    "        for c_idx, child in enumerate(children):\n",
    "            # line 18: `\\tilde{h}`\n",
    "            zeros = tf.zeros([size, self.encode_dim], tf.float32)\n",
    "            batch_children_index = [batch_index[i] for i in children_index[c_idx]]\n",
    "\n",
    "            # line 19: n\n",
    "            # make a recursive call for each child to get the output of shape\n",
    "            # (1 x self.encode_dim)\n",
    "            tree = self.traverse(child, batch_children_index)\n",
    "            if tree is None:\n",
    "                continue\n",
    "\n",
    "            children_index_instance = tf.expand_dims(children_index[c_idx], axis=-1)\n",
    "            indices = tf.Variable(children_index_instance, tf.float32)\n",
    "            batch_current += tf.tensor_scatter_nd_update(zeros, indices, tree)\n",
    "\n",
    "        return batch_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7e75f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class BatchTreeEncoder(BatchTreeEncoder):\n",
    "    def _update_node_list(self, batch_current, batch_index):\n",
    "        b_in = tf.Variable(batch_index)\n",
    "        b_in = tf.expand_dims(b_in, axis=-1)\n",
    "        self.node_list.append(tf.tensor_scatter_nd_update(self.batch_node, b_in, batch_current))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db495199",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class BatchTreeEncoder(BatchTreeEncoder):\n",
    "    def __call__(self, inputs, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.node_list = []\n",
    "        self.batch_node = tf.zeros((self.batch_size, self.encode_dim), tf.float32)\n",
    "        self.traverse(inputs, list(range(self.batch_size)))\n",
    "        self.node_list = tf.stack(self.node_list)\n",
    "        return tf.reduce_max(self.node_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f92a49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Code clone detection with code embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d712f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As described initially in this section, code clone detection can be implemented by calculating the vector embeddings for two code snippets $r_1$ and $r_2$, then measuring their distance $r = |r_1 - r_2$. A model is then trained for $\\hat{x} = W_o r + b_o$ with output $\\hat{y} = sigmoid(\\hat{x}) \\in [0, 1]$ such that the binary cross-entropy for a labelled dataset of code clones is minimized. The following code implements the model as described in the ASTNN paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21a5fc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "import tensorflow as tf\n",
    "from typing import Optional\n",
    "import numpy.typing\n",
    "\n",
    "\n",
    "class AstnnCloneDetection(tf.keras.Model):\n",
    "    def __init__(self, vocab_embedding_dim: int, hidden_dim: int, vocab_size: int, encode_dim: int, label_count: int, batch_size: int = 64, pretrained_weight: Optional[numpy.typing.NDArray] = None):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = vocab_embedding_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        self.label_count = label_count\n",
    "        self.encoder = BatchTreeEncoder(\n",
    "            vocab_size,\n",
    "            self.embedding_dim,\n",
    "            self.encode_dim,\n",
    "            self.batch_size,\n",
    "            pretrained_weight,\n",
    "        )\n",
    "        self.bigru = keras.layers.Bidirectional(keras.layers.GRU(self.hidden_dim, return_sequences=True))\n",
    "        self.hidden_state: list[tf.Tensor] = []\n",
    "\n",
    "        self.l1_layer = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))\n",
    "        self.output_layer = keras.layers.Dense(self.label_count, input_shape=(self.hidden_dim * 2,), activation=keras.activations.sigmoid)\n",
    "\n",
    "        self._reset_RNN_hidden_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78aded0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class AstnnCloneDetection(AstnnCloneDetection):\n",
    "    def _reset_RNN_hidden_state(self) -> None:\n",
    "        self.hidden_state = [tf.zeros([self.batch_size, self.hidden_dim]) for _ in range(2)]\n",
    "\n",
    "    def _setup_for_next_batch(self, batch_size: int) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self._reset_RNN_hidden_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d451d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representation of a Statement Sequence\n",
    "\n",
    "- $T$ statements of a code snippet given in $x$\n",
    "- each statement is encoded using the `BatchTreeEncoder` and placed on the `result_stack`\n",
    "- Gated Recurrent Unit (GRU) in both directions over encoded statements $e_i$ to learn about relation to statements before and after in the code\n",
    "$$\n",
    "  \\begin{align}\n",
    "  \t\\overrightarrow{h_t} &= \\overrightarrow{\\text{GRU}}(e_t) \\\\\n",
    "  \t\\overleftarrow{h_t} &= \\overleftarrow{\\text{GRU}}(e_t) \\\\\n",
    "  \th_t &:= \\left[ \\overleftarrow{h_t}, \\overrightarrow{h_t} \\right]\n",
    "  \\end{align}\n",
    "$$\n",
    "- reduce vectors to most important features by max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc04b19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class AstnnCloneDetection(AstnnCloneDetection):\n",
    "    def encode(self, x: tf.Tensor):\n",
    "        lengths = [len(item) for item in x]\n",
    "\n",
    "        # statement trees to encode\n",
    "        encodes = [statement_tree for code in x for statement_tree in code]\n",
    "\n",
    "        # line 4: pass the statement trees to the batch tree encoder\n",
    "        encoded = self.encoder(encodes, sum(lengths))\n",
    "        # line 24: collect onto S\n",
    "        result_stack = self._collect_stack(lengths, encoded)\n",
    "\n",
    "        # line 5: get BV\n",
    "        gru_out = self.bigru(result_stack, self.hidden_state)\n",
    "        gru_out = tf.transpose(gru_out, perm=[0, 2, 1])\n",
    "        gru_out = tf.reduce_max(gru_out, axis=[2], keepdims=True)\n",
    "        gru_out = tf.squeeze(gru_out, 2)\n",
    "\n",
    "        return gru_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74aca1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class AstnnCloneDetection(AstnnCloneDetection):\n",
    "    def _collect_stack(self, lengths: list[int], encoded: tf.Tensor) -> tf.Tensor:\n",
    "        max_length = max(lengths)\n",
    "        result_stack_tmp: list[tf.Tensor] = []\n",
    "        start: int = 0\n",
    "        end: int = 0\n",
    "        for length in lengths:\n",
    "            end += length\n",
    "            if max_length - length:\n",
    "                filler = tf.zeros((max_length - length, self.encode_dim))\n",
    "                result_stack_tmp.append(filler)\n",
    "            result_stack_tmp.append(encoded[start:end])\n",
    "            start = end\n",
    "\n",
    "        # reshape the stack S to be usable as input for the GRU\n",
    "        result_stack = tf.concat(result_stack_tmp, axis=0)\n",
    "        return tf.reshape(result_stack, [self.batch_size, max_length, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d206bf41",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class AstnnCloneDetection(AstnnCloneDetection):\n",
    "    def call(self, inputs, training=None, mask=None, **kwargs):\n",
    "        code1, code2 = inputs\n",
    "        self._setup_for_next_batch(batch_size=1)\n",
    "        vec1, vec2 = self.encode(code1), self.encode(code2)\n",
    "        return self.output_layer(self.l1_layer([vec1, vec2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd6231c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making a Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bde850",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To see the model in action, of course we need to train it on a large dataset. A labelled dataset of code clones is available  in the [BigCloneBench](https://github.com/clonebench/BigCloneBench) dataset. ASTNN was trained on this dataset, and we will simply load the vocabulary and model trained by the authors of ASTNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e679ab7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v = Word2Vec.load(\"data/astnn/w2v_128\")\n",
    "\n",
    "def load_model() -> AstnnCloneDetection:\n",
    "    vocab_size = len(w2v.wv.vectors) + 1\n",
    "    w2v_embeddings = numpy.zeros((vocab_size, w2v.vector_size), dtype=float)\n",
    "    w2v_embeddings[: w2v.wv.vectors.shape[0]] = w2v.wv.vectors\n",
    "    model = AstnnCloneDetection(vocab_embedding_dim=128, hidden_dim=100, vocab_size=vocab_size, encode_dim=128, label_count=1, batch_size=1, pretrained_weight=w2v_embeddings)\n",
    "    dummy = [[[33, [2, [30, [40, [4]]]]]]]\n",
    "    x = model((dummy, dummy)) # Tensorflow lazy init: force initialisation using dummy data\n",
    "    model.load_weights(\"./data/astnn/weights/weights\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801b83e9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a78f3a4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "With this model, we can use any new pair of C code snippets and query the predicted label (0 = no clones, 1 = clones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39b237",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model: AstnnCloneDetection, code1: str, code2: str) -> float:\n",
    "    code_parser = pycparser.c_parser.CParser()\n",
    "    c1 = code_parser.parse(code1)\n",
    "    c2 = code_parser.parse(code2)\n",
    "\n",
    "    code1, code2 = to_statement_trees(c1), to_statement_trees(c2)\n",
    "    output = model(([code1], [code2]))\n",
    "    return output[-1][-1].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cdc854",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's define some usual example code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efc0e08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "code1 = \"\"\"\n",
    "int foo(int x) {\n",
    "  if (x > 0) {\n",
    "      printf(\"Hallo\");\n",
    "  } else {\n",
    "      printf(\"Nicht hallo\");  \n",
    "  }\n",
    "  return 0;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e841c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "code2 = \"\"\"\n",
    "int bar(int x) {\n",
    "  if (x > 0) {\n",
    "      printf(\"Hallo\");\n",
    "  } else {\n",
    "      printf(\"Nicht hallo\");  \n",
    "  }\n",
    "  return 0;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746d5c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "code3 = \"\"\"\n",
    "int bar(int x) {\n",
    "  printf(\"Not a clone\");\n",
    "  return 0;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0327a8f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For example, the first and second code snippet are identical except for the function name, so we would expect it to be detected as a clone pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2543b563",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "predict(model, code1, code2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7032cc40",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In contrast, the first and third snippet represent entirely different code and should thus not be detected as a clone pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c9a31",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predict(model, code1, code3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c0990",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The prediction is turned into a label by using a threshold $\\delta$, such that the two code snippets are a clone pair if the prediction $p > \\delta$. For example, the ASTNN experiments set $\\delta = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a50916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
